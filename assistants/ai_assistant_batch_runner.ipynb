{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "058e979c",
   "metadata": {},
   "source": [
    "# ü§ñ AI Assistant Colab Backend - Batch Mode with Assistant Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9dd3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8832ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "CONFIG_DIR = '/content/drive/MyDrive/assistant_configs'\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/assistant_outputs'\n",
    "\n",
    "os.makedirs(CONFIG_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print('‚úÖ Folders are ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b189ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def run_web_scraper(config):\n",
    "    url = config.get('url', '')\n",
    "    filters = [f.strip() for f in config.get('filters', '').split(',') if f.strip()]\n",
    "\n",
    "    print(f'üåê Scraping: {url}')\n",
    "    try:\n",
    "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        items = []\n",
    "        for item in soup.select('.item-cell'):\n",
    "            title = item.select_one('.item-title')\n",
    "            price = item.select_one('.price-current')\n",
    "            rating = item.select_one('.item-rating')\n",
    "\n",
    "            data = {\n",
    "                'title': title.text.strip() if title else None,\n",
    "                'price': price.text.strip() if price else None,\n",
    "                'rating': rating['title'] if rating and rating.has_attr('title') else None\n",
    "            }\n",
    "            filtered = {k: v for k, v in data.items() if k in filters or not filters}\n",
    "            items.append(filtered)\n",
    "\n",
    "        df = pd.DataFrame(items)\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"web_scraper_output_{timestamp}.csv\"\n",
    "        output_path = os.path.join(OUTPUT_DIR, filename)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f'‚úÖ Scraped {len(items)} items | Saved to {output_path}')\n",
    "    except Exception as e:\n",
    "        print(f'‚ùå Error scraping {url}: {e}')\n",
    "\n",
    "def run_api_fetcher(config):\n",
    "    print(f\"‚öôÔ∏è Running API Fetcher (Placeholder)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057eac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÅ Batch Mode Toggle\n",
    "batch_mode = True\n",
    "\n",
    "# Process each config in folder\n",
    "config_files = [f for f in os.listdir(CONFIG_DIR) if f.endswith('.json')]\n",
    "if not config_files:\n",
    "    raise FileNotFoundError('‚ùå No config files found.')\n",
    "\n",
    "for file in config_files:\n",
    "    config_path = os.path.join(CONFIG_DIR, file)\n",
    "    print(f'\\nüìÑ Processing config: {file}')\n",
    "    try:\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        task_type = config.get('task_type')\n",
    "        if task_type == 'web_scraper':\n",
    "            run_web_scraper(config)\n",
    "        elif task_type == 'api_fetcher':\n",
    "            run_api_fetcher(config)\n",
    "        else:\n",
    "            print(f'‚ö†Ô∏è Unknown task_type: {task_type}')\n",
    "    except Exception as e:\n",
    "        print(f'‚ùå Failed to process {file}: {e}')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
