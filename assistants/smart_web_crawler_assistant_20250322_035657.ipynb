{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1fbdc50",
   "metadata": {},
   "source": [
    "# üß† Smart Web Crawler Assistant ‚Äì Full Colab Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2402ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43043ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "CONFIG_DIR = '/content/drive/MyDrive/assistant_configs'\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/assistant_outputs'\n",
    "\n",
    "os.makedirs(CONFIG_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print('‚úÖ Folders ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c773f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# List available config files\n",
    "config_files = [f for f in os.listdir(CONFIG_DIR) if f.endswith('.json')]\n",
    "if not config_files:\n",
    "    raise FileNotFoundError('‚ùå No config files found in assistant_configs.')\n",
    "\n",
    "print('üìÅ Available Config Files:')\n",
    "for i, file in enumerate(config_files):\n",
    "    print(f\"{i+1}: {file}\")\n",
    "\n",
    "selection = int(input('Select a config file number: ')) - 1\n",
    "CONFIG_PATH = os.path.join(CONFIG_DIR, config_files[selection])\n",
    "\n",
    "try:\n",
    "    with open(CONFIG_PATH, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    print('‚úÖ Config Loaded Successfully!')\n",
    "except FileNotFoundError:\n",
    "    print('‚ùå File not found.')\n",
    "except json.JSONDecodeError:\n",
    "    print('‚ùå Invalid JSON format.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0168691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = config.get('prompt', '')\n",
    "url = config.get('url', '')\n",
    "filters = [f.strip() for f in config.get('filters', '').split(',') if f.strip()]\n",
    "\n",
    "print(f\"Prompt: {prompt}\\nURL: {url}\\nFilters: {filters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90861e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for scraping logic using requests/BeautifulSoup or Scrapy\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Example logic (customize per site)\n",
    "items = []\n",
    "for item in soup.select('.item-cell'):\n",
    "    title = item.select_one('.item-title')\n",
    "    price = item.select_one('.price-current')\n",
    "    data = {\n",
    "        'title': title.text.strip() if title else None,\n",
    "        'price': price.text.strip() if price else None,\n",
    "    }\n",
    "    filtered = {k: v for k, v in data.items() if k in filters or not filters}\n",
    "    items.append(filtered)\n",
    "\n",
    "print(f'‚úÖ Scraped {len(items)} items.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029bfdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "df = pd.DataFrame(items)\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_path = os.path.join(OUTPUT_DIR, f'smart_web_crawler_output_{timestamp}.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f'üìÅ Output saved to: {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6ceb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
